# zz_multi_proxy_pool_final_v5_api_fix.py
import configparser
import time
import itertools
import logging
import threading
import multiprocessing
import socket
import struct
import json
import os 
import sys 
from http.server import HTTPServer, BaseHTTPRequestHandler
import socketserver
import requests
import socks
import select
from urllib.parse import urlparse
import concurrent.futures 
from requests.packages.urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

# --- 0. é…ç½®æ—¥å¿— ---
def setup_logging(log_file="log.txt", console_level=logging.INFO, file_level=logging.DEBUG):
    root_logger = logging.getLogger()
    root_logger.setLevel(file_level) 
    formatter = logging.Formatter('%(asctime)s [%(levelname)s] (%(processName)s) %(message)s', datefmt='%H:%M:%S')

    if root_logger.hasHandlers():
        root_logger.handlers.clear()

    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(file_level) 
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(console_level)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

# --- 1. ä»£ç†æ± ç®¡ç†ç±» (æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ - é›†æˆ API è§£æå’Œé€»è¾‘åˆ†ç¦») ---
class ProxyPoolManager:
    def __init__(self, config_file='config.ini'):
        self._load_config(config_file)
        
        self.all_proxies = [] 
        self.available_proxies = [] 
        self.lock = threading.Lock() 
        
        self.current_proxy = None
        self.last_switch_time = time.time()
        self.request_counter = 0
        
        self.state_file = 'proxy_state.json'
        self.last_state_mtime = 0
        self.process_name = multiprocessing.current_process().name

    def _load_config(self, file_path):
        config = configparser.ConfigParser()
        try:
            config.read(file_path, encoding='utf-8') 
            self.http_port = config.getint('ProxyPool', 'HTTP_LISTEN_PORT', fallback=8888)
            self.socks5_port = config.getint('ProxyPool', 'SOCKS5_LISTEN_PORT', fallback=10808)
            self.switch_mode = config.get('ProxyPool', 'SWITCH_MODE', fallback='TIME').upper()
            self.time_interval = config.getint('ProxyPool', 'TIME_INTERVAL_SECONDS', fallback=300)
            self.request_limit = config.getint('ProxyPool', 'REQUEST_COUNT_LIMIT', fallback=100)
            self.proxy_file = config.get('ProxyPool', 'PROXY_FILE', fallback='proxies.txt')
            
            # --- API æŠ“å–é…ç½® ---
            self.api_url = config.get('ProxyPool', 'API_URL', fallback='')
            
            self.availability_check_url = config.get('HealthCheck', 'AVAILABILITY_CHECK_URL', fallback='https://www.baidu.com')
            self.health_check_interval = config.getint('HealthCheck', 'HEALTH_CHECK_INTERVAL_SECONDS', fallback=120)
            
            # --- API æŠ“å–é—´éš” ---
            fetch_config = config['ProxyFetch'] if 'ProxyFetch' in config else config['HealthCheck']
            self.fetch_proxy_interval = fetch_config.getint('FETCH_PROXY_INTERVAL_SECONDS', fallback=120)
            
        except Exception as e:
            logging.warning(f"é…ç½®æ–‡ä»¶è¯»å–éƒ¨åˆ†å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")

    def _load_proxies_from_file(self):
        """åªä»æ–‡ä»¶åŠ è½½ï¼Œç”¨äºåˆå§‹åŒ–å’Œæ›´æ–° all_proxies"""
        proxies = []
        if os.path.exists(self.proxy_file):
            try:
                with open(self.proxy_file, 'r', encoding='utf-8') as f:
                    proxies = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
                
            except Exception as e:
                logging.error(f"[Init] è¯»å–ä»£ç†æ–‡ä»¶å¤±è´¥: {e}")
        
        with self.lock:
             self.all_proxies = proxies
        if proxies:
             logging.info(f"[Init] ä» {self.proxy_file} åŠ è½½äº† {len(proxies)} ä¸ªä»£ç†ã€‚")
        return bool(proxies)

    def _save_proxies_to_file(self, proxies_list):
        """å°†æœ€æ–°çš„ä»£ç†åˆ—è¡¨å†™å…¥æ–‡ä»¶"""
        try:
            with open(self.proxy_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(list(set(proxies_list)))))
            logging.info(f"[Save] æˆåŠŸå°† {len(proxies_list)} ä¸ªä»£ç†å†™å…¥ {self.proxy_file}")
        except Exception as e:
            logging.error(f"[Save] å†™å…¥ä»£ç†æ–‡ä»¶å¤±è´¥: {e}")

    # --- çŠ¶æ€åŒæ­¥ï¼ˆä¿æŒä¸å˜ï¼‰ ---
    def _save_state(self):
        try:
            state = {
                'current_proxy': self.current_proxy,
                'timestamp': time.time(),
                'pid': os.getpid()
            }
            temp_file = self.state_file + '.tmp'
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(state, f)
            os.replace(temp_file, self.state_file)
        except Exception as e:
            logging.error(f"[State] ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")

    def _sync_state(self):
        if not os.path.exists(self.state_file):
            return

        try:
            mtime = os.path.getmtime(self.state_file)
            if mtime > self.last_state_mtime:
                self.last_state_mtime = mtime
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                
                external_proxy = state.get('current_proxy')
                if external_proxy and external_proxy != self.current_proxy:
                    logging.info(f"[Sync] åŒæ­¥åˆ°å¤–éƒ¨è¿›ç¨‹çš„æ–°ä»£ç†: {external_proxy}")
                    with self.lock:
                        self.current_proxy = external_proxy
                        self.last_switch_time = time.time()
                        self.request_counter = 0
        except Exception:
            pass

    def _check_proxy_availability(self, proxy_url):
        """æ ¸å¿ƒæ£€æŸ¥é€»è¾‘ï¼šä½¿ç”¨ä¸¥æ ¼çš„è¶…æ—¶æ§åˆ¶"""
        parsed = urlparse(proxy_url)
        scheme = parsed.scheme.lower()
        test_url = self.availability_check_url
        
        proxies = {}
        try:
            # ä¿®æ­£ SOCKS ä»£ç†çš„è¯·æ±‚æ ¼å¼
            if scheme == 'socks5':
                proxies = {'http': f'socks5h://{parsed.hostname}:{parsed.port}', 
                           'https': f'socks5h://{parsed.hostname}:{parsed.port}'}
            elif scheme == 'socks4':
                proxies = {'http': f'socks4a://{parsed.hostname}:{parsed.port}', 
                           'https': f'socks4a://{parsed.hostname}:{parsed.port}'}
            else:
                proxies = {'http': proxy_url, 'https': proxy_url}
            
            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„è¶…æ—¶æ§åˆ¶ (3ç§’è¿æ¥ï¼Œ7ç§’è¯»å–)ï¼Œé˜²æ­¢ä»£ç†é•¿æœŸé˜»å¡
            logging.debug(f"[Check] æ­£åœ¨æµ‹è¯•ä»£ç†: {proxy_url}")
            resp = requests.get(test_url, proxies=proxies, timeout=(3, 7), verify=False)
            logging.debug(f"[Check] ä»£ç† {proxy_url} æµ‹è¯•é€šè¿‡, çŠ¶æ€ç : {resp.status_code}")
            return resp.status_code >= 200 and resp.status_code < 400
        except Exception as e:
            logging.debug(f"[Check] ä»£ç† {proxy_url} æµ‹è¯•å¤±è´¥: {e}")
            return False

    def _run_health_check(self):
        """ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œå¥åº·æ£€æŸ¥ï¼Œé˜²æ­¢å¡é¡¿"""
        # --- æ£€æŸ¥æºï¼šä»…ä» self.all_proxies (æœ¬åœ°æ–‡ä»¶) åŠ è½½ ---
        self._load_proxies_from_file() 
        check_list = list(self.all_proxies)
        
        if not check_list:
            logging.warning("[Health] æ€»ä»£ç†æ± ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¥åº·æ£€æŸ¥ã€‚")
            return

        logging.info(f"[Health] å¼€å§‹å¥åº·æ£€æŸ¥... æ€»æ•°: {len(check_list)}")
        valid_proxies = []
        
        # é…ç½®çº¿ç¨‹æ± å‚æ•°
        MAX_WORKERS = 10 
        # ä¼˜åŒ–ï¼šæ€»è¶…æ—¶æ—¶é—´ç¼©çŸ­ä¸º 5 ç§’ï¼Œå¿«é€Ÿè·³è¿‡å¡é¡¿ä»£ç†
        TOTAL_CHECK_TIMEOUT = 5 

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_proxy = {executor.submit(self._check_proxy_availability, p): p for p in check_list}
            
            try:
                for future in concurrent.futures.as_completed(future_to_proxy, timeout=TOTAL_CHECK_TIMEOUT):
                    proxy = future_to_proxy[future]
                    try:
                        is_available = future.result()
                        if is_available:
                            valid_proxies.append(proxy)
                    except Exception as exc:
                        logging.debug(f"[Check] ä»£ç† {proxy} æ£€æŸ¥æŠ›å‡ºå¼‚å¸¸: {exc}")

            except concurrent.futures.TimeoutError:
                 logging.warning(f"[Health] å¥åº·æ£€æŸ¥è¶…æ—¶ ({TOTAL_CHECK_TIMEOUT}ç§’)ï¼Œè·³è¿‡å‰©ä½™æ£€æŸ¥ã€‚")
            except Exception as e:
                 logging.error(f"[Health] çº¿ç¨‹æ± æ‰§è¡Œå¼‚å¸¸: {e}")

        with self.lock:
            self.available_proxies = valid_proxies
            if not self.available_proxies:
                logging.warning("[Health] æ— å¯ç”¨ä»£ç†ï¼Œè¿›å…¥å…œåº•æ¨¡å¼ï¼ˆä½¿ç”¨å…¨åˆ—è¡¨ï¼‰ã€‚")
            else:
                logging.info(f"[Health] æ£€æŸ¥å®Œæˆï¼Œå¯ç”¨: {len(self.available_proxies)}/{len(self.all_proxies)}")

    def _health_check_loop(self):
        while True:
            try:
                self._run_health_check()
            except Exception as e:
                logging.error(f"[Health] Loop error: {e}")
            time.sleep(self.health_check_interval)

    # --- ä¿®å¤ï¼šAPI æŠ“å–é€»è¾‘ï¼ˆå¤„ç† JSON åµŒå¥—ç»“æ„ï¼‰ ---
    def _fetch_proxies_from_api(self):
        """ä»é…ç½®çš„ API åœ°å€æ‹‰å–æ–°ä»£ç†ï¼Œè§£æåµŒå¥— JSONï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
        if not self.api_url:
            return

        logging.info("[Fetch] æ­£åœ¨ä» API æ‹‰å–æ–°ä»£ç†...")
        try:
            resp = requests.get(self.api_url, timeout=10)
            resp.raise_for_status() 

            new_proxies_list = []
            
            # --- æ ¸å¿ƒè§£æé€»è¾‘ä¿®å¤ ---
            try:
                data = resp.json()
                # æ£€æŸ¥æ‚¨æä¾›çš„ç»“æ„: {"success":true,"free":{"proxies": [...]}}
                if isinstance(data, dict) and data.get('success') and 'free' in data:
                    proxy_container = data['free']
                    if isinstance(proxy_container, dict) and 'proxies' in proxy_container:
                        for p in proxy_container['proxies']:
                            # æ ¼å¼åŒ–ä¸º scheme://ip:port
                            if all(key in p for key in ['ip', 'port', 'scheme']):
                                new_proxies_list.append(f"{p['scheme']}://{p['ip']}:{p['port']}")
                            
            except json.JSONDecodeError:
                # å¤„ç†é JSON å“åº”ï¼Œä½œä¸ºæ–‡æœ¬å¤„ç†
                new_proxies_list = [p.strip() for p in resp.text.splitlines() if p.strip()]

            if new_proxies_list:
                
                # 1. è¯»å–å½“å‰æ–‡ä»¶ä¸­çš„ä»£ç†
                current_proxies = set(self.all_proxies) 
                
                # 2. åˆå¹¶å»é‡
                old_count = len(current_proxies)
                current_proxies.update(new_proxies_list)
                
                # 3. ä¿å­˜åˆ°æ–‡ä»¶
                self._save_proxies_to_file(list(current_proxies))
                
                added_count = len(current_proxies) - old_count
                
                if added_count > 0:
                    logging.info(f"[Fetch] æˆåŠŸè§£æ {len(new_proxies_list)} ä¸ªä»£ç†ï¼Œæ–°å¢ {added_count} ä¸ªï¼Œæ€»æ± æ›´æ–°ä¸º {len(current_proxies)} ä¸ªã€‚")
                else:
                    logging.info("[Fetch] API ä»£ç†å·²å­˜åœ¨æˆ–æ— æ–°å¢ã€‚")

            # 4. æŠ“å–å®Œæˆåï¼Œæ›´æ–°å†…å­˜ä¸­çš„ self.all_proxies
            self._load_proxies_from_file()

        except requests.RequestException as e:
            logging.error(f"[Fetch] ä» API æ‹‰å–ä»£ç†å¤±è´¥: {e}")

    def _fetch_proxy_loop(self):
        if not self.api_url:
            return
            
        while True:
            try:
                self._fetch_proxies_from_api()
            except Exception as e:
                logging.error(f"[Fetch] Loop error: {e}")
            time.sleep(self.fetch_proxy_interval)

    def initial_setup(self):
        # 1. å¯åŠ¨æ—¶åŠ è½½æœ¬åœ°ä»£ç†æ–‡ä»¶
        self._load_proxies_from_file()
        
        if not self.all_proxies and not self.api_url:
            logging.error("æ²¡æœ‰ä»£ç†å¯ç”¨ï¼Œè¯·æ£€æŸ¥ proxies.txt æˆ–é…ç½® API_URL")
            return

        # 2. å¯åŠ¨å¥åº·æ£€æŸ¥çº¿ç¨‹ (æ£€æŸ¥é¢‘ç‡ç”± HEALTH_CHECK_INTERVAL_SECONDS æ§åˆ¶)
        t1 = threading.Thread(target=self._health_check_loop, daemon=True)
        t1.start()
        
        # 3. å¯åŠ¨ API æŠ“å–çº¿ç¨‹ (å¦‚æœé…ç½®äº† API)
        if self.api_url:
            self._fetch_proxies_from_api() # é¦–æ¬¡ç«‹å³æ‹‰å–
            t2 = threading.Thread(target=self._fetch_proxy_loop, daemon=True)
            t2.start()
        
        # 4. é¦–æ¬¡å¥åº·æ£€æŸ¥ï¼ˆä½¿ç”¨å·²åŠ è½½/æŠ“å–/åˆå¹¶åçš„ä»£ç†åˆ—è¡¨ï¼‰
        # ä¾èµ– _run_health_check ä¸­ TOTAL_CHECK_TIMEOUT=5 çš„ä¿æŠ¤
        self._run_health_check()

    # --- ä»£ç†åˆ‡æ¢å’Œè®¡æ•°é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰ ---
    def get_current_proxy(self):
        self._sync_state()

        with self.lock:
            if not self.current_proxy:
                self._pick_next_proxy()
            
            if self.switch_mode == 'TIME' and (time.time() - self.last_switch_time > self.time_interval):
                logging.info("è§¦å‘æ—¶é—´åˆ‡æ¢")
                self._pick_next_proxy()
            elif self.switch_mode == 'REQUEST' and (self.request_counter >= self.request_limit):
                logging.info("è§¦å‘è¯·æ±‚æ¬¡æ•°åˆ‡æ¢")
                self._pick_next_proxy()
                
            return self.current_proxy
    
    def _pick_next_proxy(self):
        pool = self.available_proxies if self.available_proxies else self.all_proxies
        if not pool: return

        prev = self.current_proxy
        next_proxy = None

        try:
            current_index = -1
            if self.current_proxy and self.current_proxy in pool:
                current_index = pool.index(self.current_proxy)
            
            next_index = (current_index + 1) % len(pool)
            next_proxy = pool[next_index]
        except Exception:
            next_proxy = pool[0]

        self.current_proxy = next_proxy
        
        if self.current_proxy != prev:
            logging.info(f"ğŸ”„ åˆ‡æ¢ä»£ç†: {prev} -> {self.current_proxy}")
            self.last_switch_time = time.time()
            self.request_counter = 0
            self._save_state()

    def switch_proxy(self, failed_proxy=None):
        with self.lock:
            if failed_proxy and failed_proxy in self.available_proxies:
                self.available_proxies.remove(failed_proxy)
                logging.warning(f"âŒ ä»£ç†å¤±è´¥ï¼Œä»å¯ç”¨æ± ä¸­ä¸´æ—¶ç§»é™¤: {failed_proxy}")
            
            if failed_proxy == self.current_proxy:
                self.current_proxy = None
            
            self._pick_next_proxy()

    def increment_counter(self):
        with self.lock:
            self.request_counter += 1

    def get_proxy_parts(self, proxy_url):
        if not proxy_url: return None, None, None
        try:
            p = urlparse(proxy_url)
            return p.hostname, p.port, p.scheme
        except:
            return None, None, None

# --- 2. HTTP æœåŠ¡å™¨ï¼ˆä¿æŒä¸å˜ï¼‰ ---
class HTTP_ProxyHandler(BaseHTTPRequestHandler):
    def log_message(self, format, *args):
        return

    def _get_upstream_proxy(self):
        manager = getattr(self.server, 'manager', None)
        if manager: return manager.get_current_proxy()
        return None

    def do_CONNECT(self):
        proxy_url = self._get_upstream_proxy()
        if not proxy_url:
            self.send_error(503, "No Proxy Available")
            return

        host, port, scheme = getattr(self.server, 'manager').get_proxy_parts(proxy_url)
        target_host, target_port = self.path.split(':')
        target_port = int(target_port)

        try:
            s = socks.socksocket()
            if scheme == 'socks5':
                s.set_proxy(socks.SOCKS5, host, port, rdns=True)
            elif scheme == 'socks4':
                s.set_proxy(socks.SOCKS4, host, port, rdns=True)
            
            s.settimeout(10)
            s.connect((target_host, target_port))

            self.send_response(200, 'Connection Established')
            self.end_headers()
            self._relay(self.connection, s)
        except Exception as e:
            logging.debug(f"Tunnel failed: {e}")
            self.server.manager.switch_proxy(proxy_url)
            self.send_error(502)

    def do_GET(self): self._handle_http()
    def do_POST(self): self._handle_http()

    def _handle_http(self):
        proxy_url = self._get_upstream_proxy()
        if not proxy_url:
            self.send_error(503)
            return
            
        url = self.path
        if not url.startswith('http'):
            url = f"http://{self.headers.get('Host')}{url}"

        try:
            req_proxies = {}
            if 'socks5' in proxy_url:
                fixed_url = proxy_url.replace('socks5://', 'socks5h://')
                req_proxies = {'http': fixed_url, 'https': fixed_url}
            elif 'socks4' in proxy_url:
                fixed_url = proxy_url.replace('socks4://', 'socks4a://')
                req_proxies = {'http': fixed_url, 'https': fixed_url}
            else:
                req_proxies = {'http': proxy_url, 'https': proxy_url}

            headers = dict(self.headers)
            if 'Proxy-Connection' in headers: del headers['Proxy-Connection']
            
            body = None
            if 'Content-Length' in headers:
                body = self.rfile.read(int(headers['Content-Length']))

            resp = requests.request(
                self.command, url, headers=headers, data=body,
                proxies=req_proxies, verify=False, allow_redirects=False,
                timeout=10, stream=True
            )

            self.send_response(resp.status_code)
            for k, v in resp.headers.items():
                if k.lower() not in ['transfer-encoding', 'content-encoding', 'connection']:
                    self.send_header(k, v)
            self.end_headers()
            
            for chunk in resp.iter_content(8192):
                self.wfile.write(chunk)
            self.server.manager.increment_counter()
            
        except Exception as e:
            logging.debug(f"HTTP Request failed: {e}")
            self.server.manager.switch_proxy(proxy_url)
            self.send_error(502)

    def _relay(self, client, remote):
        sockets = [client, remote]
        try:
            while True:
                r, _, _ = select.select(sockets, [], sockets, 30)
                if not r: break
                for s in r:
                    data = s.recv(8192)
                    if not data: return
                    if s is client: remote.sendall(data)
                    else: client.sendall(data)
        except: pass
        finally:
            try: client.close()
            except: pass
            try: remote.close()
            except: pass

class ThreadingHTTPServer(HTTPServer):
    def __init__(self, addr, handler, manager):
        super().__init__(addr, handler)
        self.manager = manager

# --- 3. SOCKS5 æœåŠ¡å™¨ï¼ˆä¿æŒä¸å˜ï¼‰ ---
class SOCKS5_Handler(socketserver.BaseRequestHandler):
    def handle(self):
        manager = self.server.manager
        client = self.request
        current_proxy = None 
        
        try:
            current_proxy = manager.get_current_proxy()
            if not current_proxy: 
                client.close()
                return

            client.recv(262)
            client.send(b"\x05\x00")
            
            data = client.recv(4)
            if not data or data[1] != 1: return
            
            addr_type = data[3]
            if addr_type == 1:
                addr = socket.inet_ntoa(client.recv(4))
            elif addr_type == 3:
                addr = client.recv(ord(client.recv(1))).decode()
            else: return
            
            port = struct.unpack('>H', client.recv(2))[0]
            
            phost, pport, pscheme = manager.get_proxy_parts(current_proxy)
            
            remote = socks.socksocket()
            if 'socks5' in pscheme:
                remote.set_proxy(socks.SOCKS5, phost, pport, rdns=True)
            elif 'socks4' in pscheme:
                remote.set_proxy(socks.SOCKS4, phost, pport, rdns=True)
                
            remote.settimeout(10)
            remote.connect((addr, port))
            
            client.send(b"\x05\x00\x00\x01" + socket.inet_aton("0.0.0.0") + struct.pack(">H", 0))
            
            manager.increment_counter()
            self._relay(client, remote)
        except Exception as e:
            logging.debug(f"SOCKS5 Error: {e}")
            if current_proxy:
                manager.switch_proxy(current_proxy)
            client.close()

    def _relay(self, client, remote):
        sockets = [client, remote]
        try:
            while True:
                r, _, _ = select.select(sockets, [], sockets, 30)
                if not r: break
                for s in r:
                    data = s.recv(8192)
                    if not data: return
                    if s is client: remote.sendall(data)
                    else: client.sendall(data)
        except: pass
        finally:
            remote.close()

class ThreadingSocksServer(socketserver.ThreadingTCPServer):
    allow_reuse_address = True
    def __init__(self, addr, handler, manager):
        super().__init__(addr, handler)
        self.manager = manager

# --- 4. å…¨å±€è¿›ç¨‹å¯åŠ¨å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰ ---
def run_proxy_service(service_type):
    setup_logging()
    
    manager = ProxyPoolManager()
    manager.initial_setup()

    if service_type == 'http':
        logging.info(f"ğŸš€ HTTP Proxy å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {manager.http_port}")
        server = ThreadingHTTPServer(('0.0.0.0', manager.http_port), HTTP_ProxyHandler, manager)
        server.serve_forever()
    elif service_type == 'socks':
        logging.info(f"ğŸš€ SOCKS5 Proxy å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {manager.socks5_port}")
        server = ThreadingSocksServer(('0.0.0.0', manager.socks5_port), SOCKS5_Handler, manager)
        server.serve_forever()

# --- 5. ä¸»ç¨‹åºå…¥å£ï¼ˆä¿æŒä¸å˜ï¼‰ ---
if __name__ == '__main__':
    setup_logging()
    logging.info("=== åŠ¨æ€ä»£ç†æ± æœåŠ¡å¯åŠ¨ ===")
    
    if os.path.exists('proxy_state.json'):
        try: os.remove('proxy_state.json')
        except: pass
    
    p1 = multiprocessing.Process(target=run_proxy_service, args=('http',), name="HTTP_Process")
    p2 = multiprocessing.Process(target=run_proxy_service, args=('socks',), name="SOCKS_Process")

    p1.start()
    p2.start()
    
    try:
        p1.join()
        p2.join()
    except KeyboardInterrupt:
        logging.info("Stopping services...")
        p1.terminate()
        p2.terminate()
        p1.join()
        p2.join()